% We aims at leveraging monocular depth to improve monocular 3D object detection. Related work has shown that depth from monocular images can be used to improve 3D object detection. However, the gap to methods that rely on Lidar data as input is still significant. Our goal is to show that this gap can be partially overcome by using increasing amounts of data to pretrain the depth estimation network. Our hypothesis is, unlike standard pretraining / transfer strategies (e.g. ImageNet pretraining), 3D detection requires a pretraining scheme deliberately focused on learning good depth representation. We assume the availability of a large corpus of images paired with Lidar information, which we use to pretrain networks that predict per-pixel depth given an RGB image as input. We show that the pretrained depth networks are useful for two approaches for monocular 3D detection; FCOS-3D, a novel extension of a standard 2D detector, and Patchnet, a pseudo-lidar approach that directly consumes the output of monocular depth estimation. Further, we investigate ways of fine-tuning or adapting the pretrained monodepth network, with the aim of deploying it on a much smaller target dataset for the task of monocular 3D object detection. We aim to answer the following two questions: does supervised pretraining of the depth network (i.e. using Lidar as supervision) improve monocular 3D object detection? What are critical design choices in pretraining / fine-tune protocol and 3D detector architectures for good transfer? Finally, we show that our models achieve state-of-the-art performance on KITTI3D, Nuscenes, and Cityscapes 3D.


Recent studies on monocular 3D detection showed that the major culprit for the large gap between Lidar-based methods and monocular methods is imprecise depth estimation from a single image. In this work, we further explore the correlation between monocular depth estimation and 3D detection by linking the best recipes of monocular depth estimation with 3D detection. As a result, we identify two effective methods to maximize the use of multimodal data: supervised pretraining on depth prediction using a large set of image-pointcloud pairs, and RoI-based depth prediction while finetuning. We show the efficacy of both approaches using two leading paradigms for monocular 3D detection: Pseudo-Lidar approach and single-stage 3D detectors. In addition, we propose a modified depth estimation metric that better reflects its transferability to 3D detection, and a novel single-stage 3D detection architecture that is extensible to a depth predictor. Finally, we demonstrate our approach on three different 3D detection benchmarks, contributing significant improvements especially on rare categories in long-tail distributed data.

%Inferring depth from a single image is inherently an ill-posed problem, with monocular 3D detection methods performing significantly worse than their LiDAR counterparts. We investigate the tasks of monocular depth estimation and 3D detection, establishing a positive correlation. Building on the Pseudo-Lidar detection paradigm, we quantify and correlate depth quality and detection accuracy. We propose a modified depth estimation metric that better explains the link between depth and 3D detection performance. We also test our hypothesis on single-stage detectors: we propose a novel 3D detection architecture (FCOS-3D) that jointly outputs dense depth and 3D object boxes, and show that depth pretraining significantly surpasses standard pretraining strategies (e.g. using COCO) for 3D detection. We show that depth pretraining especially improves generalizability in detecting rare classes, for the long-tail distributed data of driving scenes.  We also show that by attending to relevant regions in the scene (i.e. containing objects of interest) we further improve depth estimation performance, leading to better monocular 3D detection. 